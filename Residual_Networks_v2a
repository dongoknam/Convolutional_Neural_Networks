#study source : https://www.coursera.org/learn/convolutional-neural-networks/home/welcome

# import packages

import numpy as np
from keras import layers
from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePoolikng2D, MaxPooling2D, GlobalMaxPooling2D
from keras.models import Model, load_model
from keras.preprocessing import image
from keras.utils import layer_utils
from keras.utils.data_utils import get_file
from keras.applications.imagenet_utils import preprocess_input
import pydot
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot
from keras.utils import plot_model
from resnets_utils import *
from keras.initializers import glorot_uniform
import scipy.misc
from matplotlib.pyplot import imshow
%matplotlib inline

import keras.backend as K
K.set_image_data_format('channels_last')
K.set_learning_phase(1)


# 1. The problem of very deep neural networks - vanishing/exploding gradients


# 2. Building a Residual Network - shortcut/skip connection : stacking up the identity block (skip connection)

# 1) define the name basis
# 2) retrieve filters
# 3) Save the input value. You'll need this later to add back to the main path (shortcut)
# 4) first component of main path
# 5) second component of main path
# 6) third component of main path
# 7) final step: add shortcut value to main path, and pass it through a RELU activation
# 8) return X

def identity_block(X, f, filters, stage, block):
  """
  Implementation of the identity block as defined in Figure 4
  
  Arguments:
  X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)
  f -- integer, specifying the shape of the middle CONV's window for the main path
  filters -- python list of integers, defining the number of filters in the CONV layers of the main path
  stage -- integer, used to name the lyaers, depending on their position in the network
  block -- string/character, used to name the layers, depending on their position in the network
  
  Returns:
  X -- output of the identity block, tensor of shape (n_H, n_W, n_C)
  
  # defining name basis
  conv_name_base = 'res' + str(stage) + block + '_branch'
  bn_name_base = 'bn' + str(stage) + block + '_branch'
  
  # retrieve filters
  F1, F2, F3 = filters
  
  # save the input value.
  X_shortcut = X
  
  # first component of main path
  X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer - glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)
  X = Activation('relu')(X)
  
  # second component of main path
  X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer - glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)
  X = Activation('relu')(X)
  
  # third component of main path
  X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer - glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)
    
  # add shortcut value to main path, and pass it through a RELU activation
  X = add()[X, X_shortcut])
  X = Activation('relu')(X)
  
  return X
  
  
 # 3. Convolutional block
 
  
